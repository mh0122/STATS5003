---
title: "Group Assignment STAT5003"
author: "Moal9049"
date: "`r Sys.Date()`"
output: 
  html_document:
  code_folding: hide
---


#Introduction
Cardiovascular disease (CVD) is the leading cause of death around the world (WHO 2019). The World Health Organisation (WHO) estimated that CVD was accountable for 16% of the world’s total number of deaths during 2019, with deaths increasing more than 20% from 2000 (WHO 2019). The majority of individuals are usually diagnosed with a type of CVD after having common symptoms such as chest pain, heart attack or sudden cardiac arrest (CDC 2022). As a result, researchers and governments worldwide have been investing heavily in studying the leading indicators of CVD to reduce the death rate caused by such diseases. Epidemiological studies and randomized clinical trials have shown that CVD is largely preventable (Cooper et al., 2000). Therefore, it is important to identify individuals at risk of being diagnosed with a type of CVD at an early stage to prevent life-threatening outcomes.

This study is concerned with a supervised binary classification problem to predict whether an individual should be diagnosed with CVD or not, by building prediction models based on the history of health risk factors of individuals.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages('ggdist')
#install.packages("ROCR")
#install.packages('car')
#install.packages('sm')
#install.packages("ROSE")
#install.packages("Rtsne")
#install.packages("performanceEstimation")
#install.packages("rpart")
#install.packages("Rborist")
#install.packages("xgboost")

library(xgboost) # for xgboost model
library(Rborist)# for random forest model
library(rpart)# for decision tree model
library(performanceEstimation) #for smote
library("dplyr") # for data preping 
library(ggplot2) #for visualisations 
library(ggdist) #for visualisations 
library(corrplot) #for visualisations 
library(caret) 
library(class)
library(pROC)
library(ROCR)
library('Rtsne')
library(MASS)
library(car)
library(sm)
library(ROSE) #for over/under sampling
library(gridExtra) #for visualisations 
```

# Data Cleaning 

``` {r Read dataset, echo: TRUE}

#read data
dat <- read.csv("2015Trimmed.csv")

```



```{r clean dataset, echo: TRUE}
## selected columns
data <- dplyr::select(dat, 'X_MICHD',
                    'X_RFHYPE5',  
                    'TOLDHI2', 
                   #'X_CRACE1', (Race) good indicator, but has over %50 NAs
                    'X_CHOLCHK', 
                    'X_BMI5', 
                    'SMOKE100', 
                    'CVDSTRK3', 
                    'DIABETE3', 
                    'X_TOTINDA', 
                    'X_FRTLT1', 
                    'X_VEGLT1', 
                    'X_RFDRHV5', 
                    'HLTHPLN1', 
                    'MEDCOST', 
                    'GENHLTH', 
                    'MENTHLTH', 
                    'PHYSHLTH', 
                    'DIFFWALK', 
                    'SEX', 
                    'X_AGEG5YR', 
                    'EDUCA', 
                    'INCOME2')


## 2 Cleaning the dataset 
## 2.1 drop all records with missing N/A values
data <- data[complete.cases(data),] # reduced the dataset by 100k records





## 2.2 Modify and clean the values to be more suitable to ML algorithms

# in this part we use the codebook to clean up the values 
## link >https://www.cdc.gov/brfss/annual_data/2015/pdf/codebook15_llcp.pdf<

#####################################

## add factors for variables that are factors and clean up the factors

## X_MICHD = Respondents that have ever reported having coronary heart disease (CHD) or myocardial infarction (MI) 
## 1 = Reported having MI or CHD; 
## 2 = Did not report having MI or CHD

## update values (Did not report having MI or CHD = 0)
data[data$X_MICHD == 2,]$X_MICHD <- 0

## change to factor 
data$X_MICHD <- as.factor(data$X_MICHD)

## rename column 
data <- rename(data, HeartDiseaseorAttack = X_MICHD)

## check 
#table(data$HeartDiseaseorAttack)

#####################################

## X_RFHYPE5 = Adults who have been told they have high blood pressure by a doctor, nurse, or other health professional 
## 1 = No, 
## 2 = Yes, 
## 9 = Missing/not answered

## update values 
## 0 = No, 
## 1 = Yes, 
data[data$X_RFHYPE5 == 1,]$X_RFHYPE5 <- 0
data[data$X_RFHYPE5 == 2,]$X_RFHYPE5 <- 1

## drop not answered ie 9 
data <- data[data$X_RFHYPE5 != 9,]

## change to factor 
data$X_RFHYPE5 <- as.factor(data$X_RFHYPE5)

## rename column 
data <- rename(data, HighBP = X_RFHYPE5)

## check 
#table(data$HighBP)


#####################################


## TOLDHI2 = Adults who have been told they have high blood pressure by a doctor, nurse, or other health professional 
## 1 = Yes, 
## 2 = No, 
## 7 = Don't know, 
## 9 = Refused / missing, blank = not asked
#table(data$TOLDHI2)

## update value(s)
## 0 = No, 
## 1 = Yes, 
data[data$TOLDHI2 == 2,]$TOLDHI2 <- 0

## drop not answered ie 7 & 9 
data <- data[data$TOLDHI2 != 9,]
data <- data[data$TOLDHI2 != 7,]

## change to factor 
data$TOLDHI2 <- as.factor(data$TOLDHI2)

## rename column 
data <- rename(data, HighChol = TOLDHI2)

## check 
#table(data$HighChol)

#####################################


## X_CHOLCHK = Cholesterol check within past five years  
## 1 = Had cholesterol checked in past 5 years , 
## 2 = Did not have cholesterol checked in past 5 years , 
## 3 = Have never had cholesterol checked, 
## 9 = missing or refsued 
#table(data$X_CHOLCHK)

## update value(s)
data[data$X_CHOLCHK == 2,]$X_CHOLCHK <- 0


## drop not answered ie 7 & 9 
data <- data[data$X_CHOLCHK != 9,]
data <- data[data$X_CHOLCHK != 7,]

## change to factor 
data$X_CHOLCHK <- as.factor(data$X_CHOLCHK)

## rename column 
data <- rename(data, CholCheck = X_CHOLCHK)

## check 
#table(data$CholCheck)

#####################################


## X_BMI5 = Body Mass Index (BMI) 
## 1 - 9999 1 or greater Notes: WTKG3/(HTM4*HTM4) (Has 2 implied decimal places), 
## BLANK = refused to answer, missing
#table(data$X_BMI5)
#sum(is.na(data$X_BMI5)) #no missing 

## update value(s)
## round values (2099 => 20.99)
data$X_BMI5 <- round(data$X_BMI5/100,2)

## rename column 
data <- rename(data, BMI = X_BMI5)
org.BMI <- data[,c("BMI","HeartDiseaseorAttack")]
## check 
#table(data$BMI)

#####################################


## SMOKE100 = Have you smoked at least 100 cigarettes in your entire life?   [Note:  5 packs = 100 cigarettes] 
## 1 = Yes, 
## 2 = No, 
## 7 = Don't know, 
## 9 = Refused / missing
#table(data$SMOKE100)

## update value(s)
## 0 = No,
## 1 = Yes, 
data[data$SMOKE100 == 2,]$SMOKE100 <- 0


## drop not answered ie 7 & 9 
data <- data[data$SMOKE100 != 9,]
data <- data[data$SMOKE100 != 7,]

## change to factor 
data$SMOKE100 <- as.factor(data$SMOKE100)

## rename column 
data <- rename(data, Smoker = SMOKE100)

## check 
#table(data$Smoker)

#####################################

## CVDSTRK3 = (Ever told) you had a stroke. 
## 1 = Yes, 
## 2 = No, 
## 7 = Don't know, 
## 9 = Refused / missing
#table(data$CVDSTRK3)

## update value(s)
## 0 = No,
## 1 = Yes, 
data[data$CVDSTRK3 == 2,]$CVDSTRK3 <- 0


## drop not answered ie 7 & 9 
data <- data[data$CVDSTRK3 != 9,]
data <- data[data$CVDSTRK3 != 7,]

## change to factor 
data$CVDSTRK3 <- as.factor(data$CVDSTRK3)

## rename column 
data <- rename(data, Stroke = CVDSTRK3)

## check 
#table(data$Stroke)

#####################################

## DIABETE3 = (Ever told) you have diabetes  (If "Yes" and respondent is female, ask "Was this only when you were pregnant?". If Respondent says pre-diabetes or borderline diabetes, use response code 4.) 
## 1 = Yes, 
## 2 = Yes, but female told only during pregnancy, 
## 3 = No, 
## 4 = No, pre-diabetes or borderline diabetes, 
## 7 = Don't know, 
## 9 = Refused / missing, blank = not asked
#table(data$DIABETE3)
#sum(is.na(data$DIABETE3)) # no blank

## update value(s)
## 0 = No |OR| Yes, but female told only during pregnancy  
## 1 = No, pre-diabetes or borderline diabetes, 
## 2 = Yes, 
data[data$DIABETE3 == 2,]$DIABETE3 <- 0
data[data$DIABETE3 == 3,]$DIABETE3 <- 0
data[data$DIABETE3 == 1,]$DIABETE3 <- 2
data[data$DIABETE3 == 4,]$DIABETE3 <- 1

## drop not answered ie 7 & 9 
data <- data[data$DIABETE3 != 9,]
data <- data[data$DIABETE3 != 7,]

## change to factor 
data$DIABETE3 <- as.factor(data$DIABETE3)

## rename column 
data <- rename(data, Diabetes = DIABETE3)

## check 
#table(data$Diabetes)

#####################################

## X_TOTINDA = Adults who reported doing physical activity or exercise during the past 30 days other than their regular job
## 1 = Had physical activity or exercise , 
## 2 = No physical activity or exercise in last 30 days , 
## 9 = Don’t know/Refused/Missing 
#table(data$X_TOTINDA)
#sum(is.na(data$X_TOTINDA)) # no blank

## update value(s)
## 0 = No physical activity or exercise in last 30 days , 
## 1 = Had physical activity or exercise , 
data[data$X_TOTINDA == 2,]$X_TOTINDA <- 0

## drop not answered ie 9 
data <- data[data$X_TOTINDA != 9,]

## change to factor 
data$X_TOTINDA <- as.factor(data$X_TOTINDA)

## rename column 
data <- rename(data, PhysActivity = X_TOTINDA)

## check 
#table(data$PhysActivity)

#####################################

## _FRTLT1 = Consume Fruit 1 or more times per day 
## 1 = Consumed fruit one or more times per day , 
## 2 = Consumed fruit less than one time per day , 
## 9 = Don’t know/Refused/Missing 
#table(data$X_FRTLT1)
#sum(is.na(data$X_FRTLT1)) # no blank

## update value(s)
data[data$X_FRTLT1 == 2,]$X_FRTLT1 <- 0

## drop not answered ie 7 & 9 
data <- data[data$X_FRTLT1 != 9,]

## change to factor 
data$X_FRTLT1 <- as.factor(data$X_FRTLT1)

## rename column 
data <- rename(data, Fruits = X_FRTLT1)

## check 
#table(data$Fruits)

#####################################

## _VEGLT1 = Consume Fruit 1 or more times per day 
## 1 = Consumed vegetables one or more times per day , 
## 2 = Consumed vegetables less than one time per day , 
## 9 = Don’t know/Refused/Missing 
#table(data$X_VEGLT1)
#sum(is.na(data$X_VEGLT1)) # no blank

## update value(s)
data[data$X_VEGLT1 == 2,]$X_VEGLT1 <- 0

## drop not answered ie 7 & 9 
data <- data[data$X_VEGLT1 != 9,]

## change to factor 
data$X_VEGLT1 <- as.factor(data$X_VEGLT1)

## rename column 
data <- rename(data, Veggies = X_VEGLT1)

## check 
#table(data$Veggies)

#####################################

## X_RFDRHV5 = Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week)
## 1 = No , 
## 2 = Yes , 
## 9 = Don’t know/Refused/Missing 
#table(data$X_RFDRHV5)
#sum(is.na(data$X_RFDRHV5)) # no blank

## update value(s)
data[data$X_RFDRHV5 == 1,]$X_RFDRHV5 <- 0
data[data$X_RFDRHV5 == 2,]$X_RFDRHV5 <- 1

## drop not answered ie 7 & 9 
data <- data[data$X_RFDRHV5 != 9,]

## change to factor 
data$X_RFDRHV5 <- as.factor(data$X_RFDRHV5)

## rename column 
data <- rename(data, HeavyAlcoholConsumption = X_RFDRHV5)

## check 
#table(data$HeavyAlcoholConsumption)

#####################################

## HLTHPLN1 = Do you have any kind of health care coverage, including health insurance, prepaid plans such as HMOs, or government plans such as Medicare, or Indian Health Service?
## 1 = Yes , 
## 2 = No , 
## 7 = Don’t know/Not Sure, 
## 9 = Refused
#table(data$HLTHPLN1)
#sum(is.na(data$HLTHPLN1)) # no blank

## update value(s)
data[data$HLTHPLN1 == 2,]$HLTHPLN1 <- 0

## drop not answered ie 7 & 9 
data <- data[data$HLTHPLN1 != 9,]
data <- data[data$HLTHPLN1 != 7,]

## change to factor 
data$HLTHPLN1 <- as.factor(data$HLTHPLN1)

## rename column 
data <- rename(data, AnyHealthcare = HLTHPLN1)

## check 
#table(data$AnyHealthcare)

#####################################

## MEDCOST = Was there a time in the past 12 months when you needed to see a doctor but could not because of cost? 
## 1 = Yes , 
## 2 = No , 
## 7 = Don’t know/Not Sure, 
## 9 = Refused
#table(data$MEDCOST)
#sum(is.na(data$MEDCOST)) # no blank

## update value(s)
data[data$MEDCOST == 2,]$MEDCOST <- 0

## drop not answered ie 7 & 9 
data <- data[data$MEDCOST != 9,]
data <- data[data$MEDCOST != 7,]

## change to factor 
data$MEDCOST <- as.factor(data$MEDCOST)

## rename column 
data <- rename(data, NoDocbcCost = MEDCOST)

## check 
#table(data$NoDocbcCost)

#####################################

## GENHLTH = Would you say that in general your health is: 
## 1 = Excellent, 
## 2 = Very good, 
## 3= Good, 
## 4 = Fair, 
## 5= Poor, 
## 7 = Don't know/Not Sure, 
## 9 = Refused, BLANK = Not Aksed
#table(data$GENHLTH)
#sum(is.na(data$GENHLTH)) # no blank

## drop not answered ie 7 & 9 
data <- data[data$GENHLTH != 9,]
data <- data[data$GENHLTH != 7,]

## change to factor 
data$GENHLTH <- as.factor(data$GENHLTH)

## rename column 
data <- rename(data, GenHlth = GENHLTH)

## check 
#table(data$GenHlth)

#####################################

## MENTHLTH = Was there a time in the past 12 months when you needed to see a doctor but could not because of cost? 
## 1 - 30 = Number of days  , 
## 88 = None , 
## 77 = Don’t know/Not Sure, 
## 99 = Refused
#table(data$MENTHLTH)
#sum(is.na(data$MENTHLTH)) # no blank

## update value(s)
data[data$MENTHLTH == 88,]$MENTHLTH <- 0

## drop not answered ie 7 & 9 
data <- data[data$MENTHLTH != 99,]
data <- data[data$MENTHLTH != 77,]

## change to factor 
data$MENTHLTH <- as.numeric(data$MENTHLTH)

## rename column 
data <- rename(data, MentHlth = MENTHLTH)

## check 
#table(data$MentHlth)

#####################################

## PHYSHLTH = Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?
## 1 - 30 = Number of days  , 
## 88 = None , 
## 77 = Don’t know/Not Sure, 
## 99 = Refused
#table(data$PHYSHLTH)
#sum(is.na(data$PHYSHLTH)) # no blank

## update value(s)
data[data$PHYSHLTH == 88,]$PHYSHLTH <- 0

## drop not answered ie 7 & 9 
data <- data[data$PHYSHLTH != 99,]
data <- data[data$PHYSHLTH != 77,]

## change to factor 
data$PHYSHLTH <- as.numeric(data$PHYSHLTH)

## rename column 
data <- rename(data, PhysHlth = PHYSHLTH)

## check 
#table(data$PhysHlth)

#####################################

## DIFFWALK = Do you have serious difficulty walking or climbing stairs? 
## 1 = Yes , 
## 2 = No , 
## 7 = Don’t know/Not Sure, 
## 9 = Refused
#table(data$DIFFWALK)
#sum(is.na(data$DIFFWALK)) # no blank

## update value(s)
data[data$DIFFWALK == 2,]$DIFFWALK <- 0

## drop not answered ie 7 & 9 
data <- data[data$DIFFWALK != 9,]
data <- data[data$DIFFWALK != 7,]

## change to factor 
data$DIFFWALK <- as.factor(data$DIFFWALK)

## rename column 
data <- rename(data, DiffWalk = DIFFWALK)

## check 
#table(data$DiffWalk)

#####################################

## SEX = Indicate sex of respondent. 
## 1 = MALE , 
## 2 = FEMALE
#table(data$SEX)
#sum(is.na(data$SEX)) # no blank

## update value(s)
data[data$SEX == 2,]$SEX <- 0

## change to factor 
data$SEX <- as.factor(data$SEX)

## rename column 
data <- rename(data, Sex = SEX)

## check 
#table(data$Sex)

#####################################

## X_AGEG5YR = Fourteen-level age category 
## 1 = 18 <= AGE <= 24  , 
## 2 = 25 <= AGE <= 29 , 
## .... 
## 13 = Age 80 or older  , 
## 14 = Don’t know/Refused/Missing 
#table(data$X_AGEG5YR)
#sum(is.na(data$X_AGEG5YR)) # no blank


## drop not answered ie 7 & 9 
data <- data[data$X_AGEG5YR != 14,]

## change to factor 
data$X_AGEG5YR <- as.factor(data$X_AGEG5YR)

## rename column 
data <- rename(data, AgeGroup = X_AGEG5YR)

## check 
#table(data$AgeGroup)

#####################################

## EDUCA = What is the highest grade or year of school you completed? 
## 1 = Never attended school or only kindergarten, 
## 2 = Grades 1 through 8 (Elementary), 
## 3= Grades 9 through 11 (Some high school), 
## 4 = Grade 12 or GED (High school graduate), 
## 5= College 1 year to 3 years (Some college or technical school), 
## 6= College 4 years or more (College graduate), 
## 9 = Refused
#table(data$EDUCA)
#sum(is.na(data$EDUCA)) # no blank


## drop not answered ie 7 & 9 
data <- data[data$EDUCA != 9,]

## change to factor 
data$EDUCA <- as.factor(data$EDUCA)

## rename column 
data <- rename(data, Education = EDUCA)

## check 
#table(data$Education)

#####################################

## INCOME2 = Is your annual household income from all sources:  (If respondent refuses at any income level, code "Refused.")
## 1 = Less than $10,000 , 
## 2 = GLess than $15,000 ($10,000 to less than $15,000), 
## 3= Less than $20,000 ($15,000 to less than $20,000) , 
## 4 = Less than $25,000 ($20,000 to less than $25,000), 
## 5= Less than $35,000 ($25,000 to less than $35,000), 
## 6= Less than $50,000 ($35,000 to less than $50,000), 
## 7 = Less than $75,000 ($50,000 to less than $75,000),  
## 8 = $75,000 or more, 77 = Don’t know/Not sure, 99 = Refused
#table(data$INCOME2)
#sum(is.na(data$INCOME2)) # no blank


## drop not answered ie 7 & 9 
data <- data[data$INCOME2 != 99,]
data <- data[data$INCOME2 != 77,]

## change to factor 
data$INCOME2 <- as.factor(data$INCOME2)

## rename column 
data <- rename(data, Income = INCOME2)

## check 
#table(data$Income)

#####################################

## Check cleaned data structure 
dim(data)

str(data)

```


### Sclaing 
``` {r Feature engineering}

#scale 
data$BMI <- (data$BMI-min(data$BMI))/(max(data$BMI) - min(data$BMI))
data$MentHlth <- (data$MentHlth-min(data$MentHlth))/(max(data$MentHlth) - min(data$MentHlth))
data$PhysHlth <- (data$PhysHlth-min(data$PhysHlth))/(max(data$PhysHlth) - min(data$PhysHlth))

```

# Basic Data Exploration

### target variable 

``` {r target variable, echo=FALSE, out.width="50%"}

#read data


# classify missing features
target <- tibble(table(data$HeartDiseaseorAttack[!is.na(data$HeartDiseaseorAttack)]))
target <- rename(target, Count = `table(data$HeartDiseaseorAttack[!is.na(data$HeartDiseaseorAttack)])`)
target$Response <- c('No','Yes')
target$tlabel <- paste0(round(target$Count/1000,1),'k')

#plot missing features
ggplot(target, aes(x = Count/1000, y = Response, fill = Response)) +
  geom_col() +
  geom_text(
    aes(label = tlabel), 
    ## make labels left-aligned
    hjust = 0.5, nudge_x = 10
  ) +
  xlim(0, 250) +
  labs(title = "Respondents reported having coronary heart disease",
              subtitle = "Ever had CHD or MI?",
              x = "Count ('000)", 
              y = "Responded",
              caption="Figure (1): Response Variable Distribution"
       )+   
  coord_flip() + 
  ## change plot appearance
  theme_minimal()
```

The dataset is very imbalanced with over 90% of cases being not having any CVD. 


### correlation analysis
``` {r correlation plot}
#convert data to numeric in order to run correlations
  #convert to factor first to keep the integrity of the data - each value will become a number rather than turn into NA
  df_cor <- data
  df_cor <- df_cor %>% mutate_if(is.character, as.factor)
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor)

  #prepare to drop duplicates and correlations of 1     
#  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  #corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  
  #select significant values  
#  corr <- subset(corr, abs(Freq) > 0.5) 
  #sort by highest correlation
#  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  #print(corr)

  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr, 
           method ='color', 
           is.corr=TRUE, 
           tl.col="black", 
           col = COL2('PuOr', 20), 
           na.label=" ", 
           number.cex = 0.5,   
           tl.cex = 0.5)
  
```

as we see, there is a positive correlation between CVDs and high blood pressure, and negative correlation between healthy lifestyle variables and CVDs. This shows that people with active lifestyle and healthy diate are less likely to have CVD. 


#### CVD and smoking 
```{r CVD and smoking}
#smokers are at more risk of having CVDs 
hadHDA <-data[data$HeartDiseaseorAttack==1,] 


ggplot(data, aes( y = Smoker, fill = HeartDiseaseorAttack))+
  geom_bar()+
  theme_minimal()
```

Smokers are more likely to have CVDs. 

#### CVD and physical activity
```{r CVD and physical activity}
#physical activity not necessarily prevent HDA
ggplot(data, aes( y = PhysActivity, fill = HeartDiseaseorAttack))+
  geom_bar()+
  theme_minimal()
```


#### CVD and Diabetes 
```{r CVD and Diabetes }
#Diabetics respondents are at more risk of being diagnosed with CDV
data %>%
  group_by(Diabetes,HeartDiseaseorAttack) %>%
  summarise(cnt = n()) %>%
  mutate(freq = formattable::percent(cnt / sum(cnt), 1))
```

People with diabetes types 1 and 2 are more likely to have CVDs 

### CVD and BMI distribution

``` {r CVD and BMI distribution}

sm.density.compare(org.BMI$BMI, org.BMI$HeartDiseaseorAttack, xlab="BMI")
  title(main="BMI Distribution by response")



## distribution 
ggplot(org.BMI, aes(x = HeartDiseaseorAttack, y = BMI, fill = HeartDiseaseorAttack)) + 
  ggdist::stat_halfeye(
    adjust = .5, 
    width = .6, 
    .width = 0, 
    justification = -.3, 
    point_colour = NA) +
  geom_point(
    size = 0.3,
    alpha = .05,
    position = position_jitter(
      seed = 5003, width = .1 )
  ) + 
  geom_boxplot(
    width = .25, 
    outlier.shape = NA
  ) + theme_minimal()+
  coord_cartesian(xlim = c(1.2, NA), clip = "off")

```

The overall of population in the database is overweight, however, people with CVD have a slightly higher BMI. 


# Visualization t-SNE
To try to understand the data better, we will try visualizing the data using t-Distributed Stochastic Neighbour Embedding, a technique to reduce dimensionality. To train the model, perplexity was set to 50, 100, 500 and 700. The visualisation should give us a hint as to whether there exist any “discoverable” patterns in the data which the model could learn. If there is no obvious structure in the data, it is more likely that the model will perform poorly.

```{r tSNE}

# Use 10% of data to compute t-SNE
tsne_subset <- 1:as.integer(0.01*nrow(data))


#prep 50
tsne50 <- Rtsne(data[tsne_subset,-c(1, 31)], perplexity = 50, theta = 0.5, pca = F, verbose = F, max_iter = 500, dims = 2, check_duplicates = F)

classes <- as.factor(data$HeartDiseaseorAttack[tsne_subset])
tsne_mat <- as.data.frame(tsne50$Y)
prep50 <- ggplot(tsne_mat, aes(x = V1, y = V2)) + geom_point(aes(color = classes)) + theme_minimal() +  ggtitle("t-SNE prep = 50") + scale_color_manual(values = c("#E69F00", "#56B4E9"))

#prep 100
tsne100 <- Rtsne(data[tsne_subset,-c(1, 31)], perplexity = 100, theta = 0.5, pca = F, verbose = F, max_iter = 500, dims = 2, check_duplicates = F)

classes <- as.factor(data$HeartDiseaseorAttack[tsne_subset])
tsne_mat <- as.data.frame(tsne100$Y)
prep100 <- ggplot(tsne_mat, aes(x = V1, y = V2)) + geom_point(aes(color = classes)) + theme_minimal() +  ggtitle("t-SNE prep = 100") + scale_color_manual(values = c("#E69F00", "#56B4E9"))


#prep 500
tsne500 <- Rtsne(data[tsne_subset,-c(1, 31)], perplexity = 500, theta = 0.5, pca = F, verbose = F, max_iter = 1000, dims = 2, check_duplicates = F)

classes <- as.factor(data$HeartDiseaseorAttack[tsne_subset])
tsne_mat <- as.data.frame(tsne500$Y)
prep500 <- ggplot(tsne_mat, aes(x = V1, y = V2)) + geom_point(aes(color = classes)) + theme_minimal() +  ggtitle("t-SNE prep = 500") + scale_color_manual(values = c("#E69F00", "#56B4E9"))

#prep 700
tsne700 <- Rtsne(data[tsne_subset,-c(1, 31)], perplexity = 700, theta = 0.5, pca = F, verbose = F, max_iter = 5000, dims = 2, check_duplicates = F)

classes <- as.factor(data$HeartDiseaseorAttack[tsne_subset])
tsne_mat <- as.data.frame(tsne700$Y)
prep700 <- ggplot(tsne_mat, aes(x = V1, y = V2)) + geom_point(aes(color = classes)) + theme_minimal() +  ggtitle("t-SNE prep = 700") + scale_color_manual(values = c("#E69F00", "#56B4E9"))

grid.arrange(
  prep50,
  prep100,
  prep500, 
  prep700,
  nrow =2,
  top = "Comparing tSNE prep"
)
```


There appears to be no separation between the two classes.

# Modeling Approach
Standard machine learning algorithms struggle with accuracy on imbalanced data for the following reasons:

ML algorithms struggle with accuracy because of the unequal distribution in dependent variable.This causes the performance of existing classifiers to get biased towards majority class. The algorithms are accuracy driven i.e. they aim to minimize the overall error to which the minority class contributes very little. ML algorithms assume that the data set has balanced class distributions. They also assume that errors obtained from different classes have same cost.

The methods to deal with this problem are widely known as ‘Sampling Methods’. Generally, these methods aim to modify an imbalanced data into balanced distribution using some mechanism. The modification occurs by altering the size of original data set and provide the same proportion of balance.

These methods have acquired higher importance after many researches have proved that balanced data results in improved overall classification performance compared to an imbalanced data set. Hence, it’s important to learn them.
```{r split into train and test}
set.seed(5003)

inTrain <- createDataPartition(data$HeartDiseaseorAttack, p = 0.8)[[1]]
train <- data[ inTrain, ]
test <- data[-inTrain, ]
paste0("training set include ",nrow(train)," observations (i.e. ",round(nrow(train)/nrow(data)*100,0),"% of total records)")

```


Below are the methods used here to treat the imbalanced dataset:

Undersampling Oversampling Synthetic Data Generation

### Undersampling
This method reduces the number of observations from majority class to make the data set balanced. This method is best to use when the data set is huge and reducing the number of training samples helps to improve run time and storage troubles.

Undersampling methods are of 2 types: Random and Informative.

Random undersampling method randomly chooses observations from majority class which are eliminated until the data set gets balanced. Informative undersampling follows a pre-specified selection criterion to remove the observations from majority class.

A possible problem with this method is that removing observations may cause the training data to lose important information pertaining to majority class.

```{r undersampling}
## Under sampling 
data_balanced_under <- ovun.sample(HeartDiseaseorAttack ~ ., data = train, method = "under")$data

```


### Oversampling
This method works with minority class. It replicates the observations from minority class to balance the data. It is also known as upsampling. Similar to undersampling, this method also can be divided into two types: Random Oversampling and Informative Oversampling.

Random oversampling balances the data by randomly oversampling the minority class. Informative oversampling uses a pre-specified criterion and synthetically generates minority class observations.

An advantage of using this method is that it leads to no information loss. The disadvantage of using this method is that, since oversampling simply adds replicated observations in original data set, it ends up adding multiple observations of several types, thus leading to overfitting.
```{r oversampling}
## oversampling
data_balanced_over <- ovun.sample(HeartDiseaseorAttack ~ ., data = train, method = "over")$data

```


### Synthetic Data Generation (SMOTE and ROSE)
In simple words, instead of replicating and adding the observations from the minority class, it overcome imbalances by generates artificial data. It is also a type of oversampling technique.

In regards to synthetic data generation, synthetic minority oversampling technique (SMOTE) is a powerful and widely used method. SMOTE algorithm draws artificial samples by choosing points that lie on the line connecting the rare observation to one of its nearest neighbors in the feature space. ROSE (random over-sampling examples) uses smoothed bootstrapping to draw artificial samples from the feature space neighbourhood around the minority class.

It is important to note that sampling techniques should only be applied to the training set and not the testing set.

Our modeling approach will involve training a single classifier on the train set with class imbalance suitably altered using each of the techniques above. Depending on which technique yields the best roc-auc score on a holdout test set, we will build subsequent models using that chosen technique.

```{r ROSE and SMOTE}
## ROSE 
data_balanced_rose <- ROSE(HeartDiseaseorAttack ~ ., data  = train)$data 

## SMOTE
data_balanced_smote <- smote(HeartDiseaseorAttack  ~ ., train)
```

# Data Preparation
‘Time’ feature does not indicate the actual time of the transaction and is more of listing the data in chronological order. Based on the data visualization above we assume that ‘Time’ feature has little or no significance in correctly classifying a fraud transaction and hence eliminate this column from further analysis.

# Logisitc Regression (glm)
Before we start using sampling let us first look at how glm performs with imbalanced data. We use the function roc.curve available in the ROSE package to gauge model performance on the test set

```{r fit glm}
#fit models
glm_fit <- glm(HeartDiseaseorAttack ~ ., data = train, family = 'binomial')
glm_fit_up <- glm(HeartDiseaseorAttack ~ ., data = data_balanced_over, family = 'binomial')
glm_fit_down <- glm(HeartDiseaseorAttack ~ ., data = data_balanced_under, family = 'binomial')
glm_fit_rose <- glm(HeartDiseaseorAttack ~ ., data = data_balanced_rose, family = 'binomial')
glm_fit_smote <- glm(HeartDiseaseorAttack ~ ., data = data_balanced_smote, family = 'binomial')

#predict models 
pred_glm <- predict(glm_fit, newdata = test, type = 'response')
pred_glm_up <- predict(glm_fit_up, newdata = test, type = 'response')
pred_glm_down <- predict(glm_fit_down, newdata = test, type = 'response')
pred_glm_rose <- predict(glm_fit_rose, newdata = test, type = 'response')
pred_glm_smote <- predict(glm_fit_smote, newdata = test, type = 'response')

#AUC
"Standard Model"
roc.curve(test$HeartDiseaseorAttack, pred_glm, plotit = FALSE)

"Oversampling Model"
roc.curve(test$HeartDiseaseorAttack, pred_glm_up, plotit = FALSE)
"Undersampling Model"
roc.curve(test$HeartDiseaseorAttack, pred_glm_down, plotit = FALSE)
"ROSE Model"
roc.curve(test$HeartDiseaseorAttack, pred_glm_rose, plotit = FALSE)
"SMOTE Model"
roc.curve(test$HeartDiseaseorAttack, pred_glm_smote, plotit = FALSE)

```

We evaluate the model performance on test data by finding the roc auc score. We see that the auc score on the original dataset is 0.849, and 0.850 on the various sampling techniques to the data.

## glm confusion matrix
```{r confusion matrix standard glm}


#ROC Curve 
par(pty = "s")
glm.roc.info <- roc(test$HeartDiseaseorAttack, pred_glm_down, plot=TRUE, legacy.axes = TRUE, percent = TRUE, xlab = "False Positive", ylab= "True Positive", col = 'red', lwd = 2)


#choose the best cutoff point 
glm.roc.df <- data.frame( 
  tpp = glm.roc.info$sensitivities , 
  fpp = (1-glm.roc.info$specificities), 
  thresholds = glm.roc.info$thresholds
  )

# 70% threshold 
glm.cutoff <- min(glm.roc.df[glm.roc.df$tpp < 70 ,"thresholds"])
predicted <- as.factor(ifelse(pred_glm_down > glm.cutoff, 1, 0))
confusionMatrix(data =predicted, reference = test$HeartDiseaseorAttack)
```


# Decision Trees (CART)

```{r decision tree}
#fit dt model
dt_fit <- rpart(HeartDiseaseorAttack ~ ., data = train)
dt_fit_up <- rpart(HeartDiseaseorAttack ~ ., data = data_balanced_over)
dt_fit_down <- rpart(HeartDiseaseorAttack ~ ., data = data_balanced_under)
dt_fit_rose <- rpart(HeartDiseaseorAttack ~ ., data = data_balanced_rose)
dt_fit_smote <- rpart(HeartDiseaseorAttack ~ ., data = data_balanced_smote)

#test model
pred_dt <- predict(dt_fit, newdata = test, method = "class")
pred_dt_up <- predict(dt_fit_up, newdata = test, method = "class")
pred_dt_down <- predict(dt_fit_down, newdata = test, method = "class")
pred_dt_rose <- predict(dt_fit_rose, newdata = test, method = "class")
pred_dt_smote <- predict(dt_fit_smote, newdata = test, method = "class")

#AUC
"Standard Model"
roc.curve(test$HeartDiseaseorAttack, pred_dt[,2], plotit = FALSE)
"Oversampling Model"
roc.curve(test$HeartDiseaseorAttack, pred_dt_up[,2], plotit = FALSE)
"Undersampling Model"
roc.curve(test$HeartDiseaseorAttack, pred_dt_down[,2], plotit = FALSE)
"ROSE Model"
roc.curve(test$HeartDiseaseorAttack, pred_dt_rose[,2], plotit = FALSE)
"SMOTE Model"
roc.curve(test$HeartDiseaseorAttack, pred_dt_smote[,2], plotit = FALSE)
```

the standard model has the worse AUC score 50%, followed by SMOTE 74.2%, while all other models scored 78% 

#Random Forest

```{r random forest}
#split target from response
x = train[, -1]
y = train[,1]

x_up = data_balanced_over[, -1]
y_up = data_balanced_over[,1]

x_down = data_balanced_under[, -1]
y_down = data_balanced_under[,1]

x_rose = data_balanced_rose[, -1]
y_rose = data_balanced_rose[,1]

x_smote = data_balanced_smote[, -1]
y_smote = data_balanced_smote[,1]

#fit models 
rf_fit <- Rborist(x, y, ntree = 1000, minNode = 20, maxLeaf = 13)
rf_fit_up <- Rborist(x_up, y_up, ntree = 1000, minNode = 20, maxLeaf = 13)
rf_fit_down <- Rborist(x_down, y_down, ntree = 1000, minNode = 20, maxLeaf = 13)
rf_fit_rose <- Rborist(x_rose, y_rose, ntree = 1000, minNode = 20, maxLeaf = 13)
rf_fit_smote <- Rborist(x_smote, y_smote, ntree = 1000, minNode = 20, maxLeaf = 13)

#predict test 
rf_pred <- predict(rf_fit, test[,-1], ctgCensus = "prob")
rf_pred_up <- predict(rf_fit_up, test[,-1], ctgCensus = "prob")
rf_pred_down <- predict(rf_fit_down, test[,-1], ctgCensus = "prob")
rf_pred_rose <- predict(rf_fit_rose, test[,-1], ctgCensus = "prob")
rf_pred_smote <- predict(rf_fit_smote, test[,-1], ctgCensus = "prob")

prob <- rf_pred$prob
prob_up <- rf_pred_up$prob
prob_down <- rf_pred_down$prob
prob_rose <- rf_pred_rose$prob
prob_smote <- rf_pred_smote$prob

#AUC 
"Standard Model"
roc.curve(test$HeartDiseaseorAttack, prob[,2], plotit = FALSE)
"Oversampling Model"
roc.curve(test$HeartDiseaseorAttack, prob_up[,2], plotit = FALSE)
"Undersampling Model"
roc.curve(test$HeartDiseaseorAttack, prob_down[,2], plotit = FALSE)
"ROSE Model"
roc.curve(test$HeartDiseaseorAttack, prob_rose[,2], plotit = FALSE)
"SMOTE Model"
roc.curve(test$HeartDiseaseorAttack, prob_smote[,2], plotit = FALSE)
```

Random forest has better performance than decision tree and logistic regression with undersamping scoring AUC 82.9%, followed by oversamping 82.7% 


#XGBoost 

```{r XGBoost}
#prep
y = as.numeric(train$HeartDiseaseorAttack)-1
y_up = as.numeric(data_balanced_over$HeartDiseaseorAttack)-1
y_down = as.numeric(data_balanced_under$HeartDiseaseorAttack)-1
y_rose = as.numeric(data_balanced_rose$HeartDiseaseorAttack)-1
y_smote = as.numeric(data_balanced_smote$HeartDiseaseorAttack)-1

#model 
xgb_fit <- xgboost(data = data.matrix(train[,-1]), 
 label = y,
 eta = 0.1,
 gamma = 0.1,
 max_depth = 10, 
 nrounds = 300, 
 objective = "binary:logistic",
 colsample_bytree = 0.6,
 verbose = 0,
 nthread = 7,
)

xgb_fit_up <- xgboost(data = data.matrix(data_balanced_over[,-1]), 
 label = y_up,
 eta = 0.1,
 gamma = 0.1,
 max_depth = 10, 
 nrounds = 300, 
 objective = "binary:logistic",
 colsample_bytree = 0.6,
 verbose = 0,
 nthread = 7,
)

xgb_fit_down <- xgboost(data = data.matrix(data_balanced_under[,-1]), 
 label = y_down,
 eta = 0.1,
 gamma = 0.1,
 max_depth = 10, 
 nrounds = 300, 
 objective = "binary:logistic",
 colsample_bytree = 0.6,
 verbose = 0,
 nthread = 7,
)

xgb_fit_rose <- xgboost(data = data.matrix(data_balanced_rose[,-1]), 
 label = y_rose,
 eta = 0.1,
 gamma = 0.1,
 max_depth = 10, 
 nrounds = 300, 
 objective = "binary:logistic",
 colsample_bytree = 0.6,
 verbose = 0,
 nthread = 7,
)

xgb_fit_smote <- xgboost(data = data.matrix(data_balanced_smote[,-1]), 
 label = y_smote,
 eta = 0.1,
 gamma = 0.1,
 max_depth = 10, 
 nrounds = 300, 
 objective = "binary:logistic",
 colsample_bytree = 0.6,
 verbose = 0,
 nthread = 7,
)
#predict 
xgb_pred <- predict(xgb_fit, data.matrix(test[,-1]))
xgb_pred_up <- predict(xgb_fit_up, data.matrix(test[,-1]))
xgb_pred_down <- predict(xgb_fit_down, data.matrix(test[,-1]))
xgb_pred_rose <- predict(xgb_fit_rose, data.matrix(test[,-1]))
xgb_pred_smote <- predict(xgb_fit_smote, data.matrix(test[,-1]))

#AUC 
roc.curve(test$HeartDiseaseorAttack, xgb_pred, plotit = TRUE)
roc.curve(test$HeartDiseaseorAttack, xgb_pred_up, plotit = FALSE)
roc.curve(test$HeartDiseaseorAttack, xgb_pred_down, plotit = FALSE)
roc.curve(test$HeartDiseaseorAttack, xgb_pred_rose, plotit = FALSE)
roc.curve(test$HeartDiseaseorAttack, xgb_pred_smote, plotit = FALSE)
```

XGBoost with standard 84.1% 

```{r confusion matrix standard XGBoost}

#ROC Curve 
par(pty = "s")
roc.info <- roc(test$HeartDiseaseorAttack, xgb_pred, plot=TRUE, legacy.axes = TRUE, percent = TRUE, xlab = "False Positive", ylab= "True Positive", col = 'red', lwd = 2)


#choose the best cutoff point 
roc.df <- data.frame( 
  tpp = roc.info$sensitivities , 
  fpp = (1-roc.info$specificities), 
  thresholds = roc.info$thresholds
  )

# 80% threshold 
XGBoost.cutoff <- 1-max(roc.df[roc.df$tpp > 0.5 ,]$thresholds)
predicted <- as.factor(ifelse(xgb_pred > XGBoost.cutoff, 1, 0))
confusionMatrix(data =predicted, reference = test$HeartDiseaseorAttack)
```


```{r features standard XGBoost}
names <- dimnames(data.matrix(train[,-1]))[[2]]

# Compute feature importance matrix
importance_matrix <- xgb.importance(names, model = xgb_fit)
# Nice graph
xgb.plot.importance(importance_matrix[1:10,])
```
#compare all models
```{r compare ROC}
par(pty = "s")
roc(test$HeartDiseaseorAttack, xgb_pred, plot=TRUE, legacy.axes = TRUE, percent = TRUE, xlab = "False Positive", ylab= "True Positive", col = 'red', lwd = 2)
plot.roc(test$HeartDiseaseorAttack, prob_up[,2], percent = TRUE, xlab = "False Positive", ylab= "True Positive", col = 'blue', lwd = 2, add = TRUE)
plot.roc(test$HeartDiseaseorAttack, pred_dt_down[,2], percent = TRUE, xlab = "False Positive", ylab= "True Positive", col = 'black', lwd = 2, add = TRUE)
plot.roc(test$HeartDiseaseorAttack, pred_glm_up, percent = TRUE, xlab = "False Positive", ylab= "True Positive", col = 'purple', lwd = 1, lty = 2, add = TRUE)
legend("bottomright", legend =c("XGBoost", "RandomForst", "Decision Tree", "Logisitc Regression"), col = c('red','blue','black','purple'), lwd = 1)
```

